---
title: "Reproducibility Report for Poli et al. (2024, Developmental Science)"
author: "Victoria Hennessy (vhennessy@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Reproducibility reports should all use this template to standardize reporting across projects. These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

[No abstract is needed.]  Each reproducibility project will have a straightforward, no frills report of the study and reproducibility results. These reports will be publicly available as supplementary material for the aggregate report(s) of the project as a whole.  Also, to maximize project integrity, the intro and methods will be written and critiqued in advance of your attempt to reproduce the results. Introductions can be just 1-2 paragraphs clarifying the main idea of the original study, the target finding for your reproducibility attempt, and any other essential information.  It will NOT have a literature review -- that is in the original publication. You can write both the introduction and the methods in past tense.  

**Clarify key analysis of interest here** 

### Justification for choice of study

My current line of research is focused on individual differences in early information seeking. To study infant learning strategies, Poli and colleagues (2020) have developed an analytic pipeline that integrates an infant-friendly visual learning (VL) task with an ideal learner model from which information-theoretic measures (e.g., predictability and information gain) can be derived for each trial an infant sees. These measures are then incorporated into a hierarchical Bayesian model fitted to infants' looking behavior (e.g. looking time, saccadic latency) in the VL task. Modeling the relationship between the trial-by-trial quantifications of predictability and information gain and infant performance allows Poli et al. (2024) to obtain estimates of individual differences in cognitive factors captured by the model, including processing speed, learning progress, and curiosity. Working through the described pipeline will help me develop the analytic toolkit and computational understanding needed for my own research program.

### Anticipated challenges

Simulating this data and analytic pipeline involves three stages: input generation, hierarchical modeling, and outcome replication. First, trial-by-trial stimulus input for the VL task will need to be generated by defining its underlying probabilistic structure and applying information theory to calculate the independent variables (predicability and information gain). Then, I will need to implement the hierachical Bayesian model by defining hyperparameters for each of the cognitive traits and use them to generate unique latent parameters for each simulated datapoint. In theory, the individual parameters can then be used to generate and govern trial-by-trial behavioral data (i.e., saccadic latency, looking time, look-aways). This part may be challenging as it involves using complex survival function(s). Finally, if I choose to replicate the GLMs used to compare the latent parameters to infants' performance in a habituation task, I will need to generate dependent habituation outcomes and confirm that the inferred latent parameters match the main finding (e.g., that infants' processing speed predicts habituation time). 

### Links

[Project Repository](https://github.com/thenness-y/poli2024)

[Original Paper](https://github.com/thenness-y/poli2024/blob/main/original_paper/Poli2023DevSci.pdf)

## Methods

### Description of the steps required to reproduce the results

Please describe all the steps necessary to reproduce the key result(s) of this study. 

### Differences from original study

Explicitly describe known differences in the analysis pipeline between the original paper and yours (e.g., computing environment). The goal, of course, is to minimize those differences, but differences may occur. Also, note whether such differences are anticipated to influence your ability to reproduce the original results.

## Project Progress Check 1

### Measure of success

Please describe the outcome measure for the success or failure of your reproduction and how this outcome will be computed.


### Pipeline progress

Earlier in this report, you described the steps necessary to reproduce the key result(s) of this study. Please describe your progress on each of these steps (e.g., data preprocessing, model fitting, model evaluation).


## Results

### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions

#### Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Key analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Reproduction Attempt

Open the discussion section with a paragraph summarizing the primary result from the key analysis and assess whether you successfully reproduced it, partially reproduced it, or failed to reproduce it.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis of the dataset, (b) assessment of the meaning of the successful or unsuccessful reproducibility attempt - e.g., for a failure to reproduce the original findings, are the differences between original and present analyses ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the reproducibility attempt (if you contacted them).  None of these need to be long.
